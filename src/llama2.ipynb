{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10947163,"sourceType":"datasetVersion","datasetId":6809087},{"sourceId":10982480,"sourceType":"datasetVersion","datasetId":6834912},{"sourceId":11009233,"sourceType":"datasetVersion","datasetId":6854092},{"sourceId":11117857,"sourceType":"datasetVersion","datasetId":6932501},{"sourceId":11154689,"sourceType":"datasetVersion","datasetId":6959613},{"sourceId":11385032,"sourceType":"datasetVersion","datasetId":7128986},{"sourceId":11760421,"sourceType":"datasetVersion","datasetId":7382941},{"sourceId":11762195,"sourceType":"datasetVersion","datasetId":7384174},{"sourceId":11782427,"sourceType":"datasetVersion","datasetId":7397432},{"sourceId":11799005,"sourceType":"datasetVersion","datasetId":7409498},{"sourceId":11810232,"sourceType":"datasetVersion","datasetId":7417479},{"sourceId":11853410,"sourceType":"datasetVersion","datasetId":7448196},{"sourceId":11898710,"sourceType":"datasetVersion","datasetId":7479609}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CHATBOT-WITH-LLAMA-2","metadata":{}},{"cell_type":"markdown","source":"## 1. Installing Necessary Libraries","metadata":{}},{"cell_type":"code","source":"# Cập nhật pip trước khi cài đặt\n!pip install -U pip  -q\n\n# Cài đặt bitsandbytes phiên bản ổn định\n# !pip install bitsandbytes==0.39.0 --index-url https://jllllll.github.io/bitsandbytes-wheels/cu121/ -q\n\n# Cài đặt phiên bản PyTorch tương thích\n# !pip install -qqq torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118  -q\n!pip install torch -q\n\n# Cài đặt transformers, peft và accelerate mới nhất\n!pip install -qqq -U git+https://github.com/huggingface/transformers.git  -q\n!pip install -qqq -U git+https://github.com/huggingface/peft.git  -q\n!pip install -qqq -U git+https://github.com/huggingface/accelerate.git  -q\n\n# Cài đặt các thư viện hỗ trợ\n!pip install -qqq datasets==2.12.0  -q\n!pip install -qqq loralib==0.1.1  -q\n!pip install -qqq einops==0.6.1  -q\n!pip install -qqq wandb  -q\n!pip install bitsandbytes==0.41.1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:23:56.765258Z","iopub.execute_input":"2025-05-24T05:23:56.765564Z","iopub.status.idle":"2025-05-24T05:26:17.556254Z","shell.execute_reply.started":"2025-05-24T05:23:56.765543Z","shell.execute_reply":"2025-05-24T05:26:17.555412Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m114.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m134.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [nvidia-cusolver-cu12]dia-cusolver-cu12]\n\u001b[1A\u001b[2K  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [datasets]3/4\u001b[0m [datasets]ess]\n\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.6 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.14 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCollecting bitsandbytes==0.41.1\n  Downloading bitsandbytes-0.41.1-py3-none-any.whl.metadata (9.8 kB)\nDownloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m111.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.41.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install bitsandbytes --upgrade -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:26:17.557688Z","iopub.execute_input":"2025-05-24T05:26:17.557943Z","iopub.status.idle":"2025-05-24T05:26:22.252425Z","shell.execute_reply.started":"2025-05-24T05:26:17.557917Z","shell.execute_reply":"2025-05-24T05:26:22.251711Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Importing Necessary Libraries","metadata":{}},{"cell_type":"code","source":"import argparse\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport bitsandbytes as bnb\nfrom datasets import load_dataset\nfrom functools import partial\nimport os\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, BitsAndBytesConfig, DataCollatorForLanguageModeling, Trainer, TrainingArguments\nfrom datasets import load_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:26:22.253349Z","iopub.execute_input":"2025-05-24T05:26:22.253563Z","iopub.status.idle":"2025-05-24T05:26:51.469981Z","shell.execute_reply.started":"2025-05-24T05:26:22.253540Z","shell.execute_reply":"2025-05-24T05:26:51.469115Z"}},"outputs":[{"name":"stderr","text":"2025-05-24 05:26:38.545150: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748064398.752475      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748064398.814564      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:26:51.471451Z","iopub.execute_input":"2025-05-24T05:26:51.471990Z","iopub.status.idle":"2025-05-24T05:26:51.476125Z","shell.execute_reply.started":"2025-05-24T05:26:51.471971Z","shell.execute_reply":"2025-05-24T05:26:51.475266Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Device configuration\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:26:51.476970Z","iopub.execute_input":"2025-05-24T05:26:51.477239Z","iopub.status.idle":"2025-05-24T05:26:51.509951Z","shell.execute_reply.started":"2025-05-24T05:26:51.477214Z","shell.execute_reply":"2025-05-24T05:26:51.509105Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## 3: Importing Llama 2 Chat HF Model","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = \"vilm/vinallama-2.7b-chat\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:26:51.511030Z","iopub.execute_input":"2025-05-24T05:26:51.511289Z","iopub.status.idle":"2025-05-24T05:26:51.522740Z","shell.execute_reply.started":"2025-05-24T05:26:51.511266Z","shell.execute_reply":"2025-05-24T05:26:51.522055Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## 4. Performing Quantization","metadata":{}},{"cell_type":"code","source":"def create_bnb_config():\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16,\n    )\n\n    return bnb_config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:26:51.523607Z","iopub.execute_input":"2025-05-24T05:26:51.523873Z","iopub.status.idle":"2025-05-24T05:26:51.536308Z","shell.execute_reply.started":"2025-05-24T05:26:51.523850Z","shell.execute_reply":"2025-05-24T05:26:51.535587Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"bnb_config = create_bnb_config()\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map={\"\": \"cuda:0\"},  # Chỉ sử dụng GPU 0\n    trust_remote_code=True,\n    use_safetensors=True,\n    quantization_config=bnb_config,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = prepare_model_for_kbit_training(model)\nmodel.gradient_checkpointing_enable()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T00:50:13.321538Z","iopub.execute_input":"2025-05-23T00:50:13.321743Z","iopub.status.idle":"2025-05-23T00:50:57.481003Z","shell.execute_reply.started":"2025-05-23T00:50:13.321720Z","shell.execute_reply":"2025-05-23T00:50:57.480093Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/682 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4664cb3c1a74cde8f9bc7d5dd7f8e9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.55G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe0d083d9bdc41e3ada5fedb47400a68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"150cb51596754969858e01f8f039d670"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"288861835baf48f692cc4abead5fb249"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2da7c16f8ac4274a59bbc823d33362e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/558 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7717ee8612684abb89cabbd795a3243a"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/qa-dataset/vinmec_qa.csv\")\n# df = df.head(10000)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T00:50:57.481995Z","iopub.execute_input":"2025-05-23T00:50:57.482300Z","iopub.status.idle":"2025-05-23T00:50:58.261645Z","shell.execute_reply.started":"2025-05-23T00:50:57.482271Z","shell.execute_reply":"2025-05-23T00:50:58.260738Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                            question  \\\n0  liều dùng và cách sử dụng loperamide như thế nào?   \n1  loperamide nên dùng bao nhiêu và dùng như thế ...   \n2  những điều cần chú ý khi sử dụng loperamide là...   \n3          có điều gì cần lưu ý khi dùng loperamide?   \n4     loperamide được chỉ định trong trường hợp nào?   \n\n                                              answer  \\\n0  Trẻ em: liều dùng tùy theo độ tuổi và tình trạ...   \n1  Trẻ em: liều dùng tùy theo độ tuổi và tình trạ...   \n2  Ngừng thuốc khi không hiệu quả trong 48 giờ, k...   \n3  Ngừng thuốc khi không hiệu quả trong 48 giờ, k...   \n4  Tổn thương gan, tắc ruột, táo bón, viêm loét đ...   \n\n                                             context  \n0  Liều và cách dùng của loperamide là người lớn:...  \n1  Liều và cách dùng của loperamide là người lớn:...  \n2  Liều và cách dùng của loperamide là người lớn:...  \n3  Liều và cách dùng của loperamide là người lớn:...  \n4  Liều và cách dùng của loperamide là người lớn:...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n      <th>context</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>liều dùng và cách sử dụng loperamide như thế nào?</td>\n      <td>Trẻ em: liều dùng tùy theo độ tuổi và tình trạ...</td>\n      <td>Liều và cách dùng của loperamide là người lớn:...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>loperamide nên dùng bao nhiêu và dùng như thế ...</td>\n      <td>Trẻ em: liều dùng tùy theo độ tuổi và tình trạ...</td>\n      <td>Liều và cách dùng của loperamide là người lớn:...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>những điều cần chú ý khi sử dụng loperamide là...</td>\n      <td>Ngừng thuốc khi không hiệu quả trong 48 giờ, k...</td>\n      <td>Liều và cách dùng của loperamide là người lớn:...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>có điều gì cần lưu ý khi dùng loperamide?</td>\n      <td>Ngừng thuốc khi không hiệu quả trong 48 giờ, k...</td>\n      <td>Liều và cách dùng của loperamide là người lớn:...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>loperamide được chỉ định trong trường hợp nào?</td>\n      <td>Tổn thương gan, tắc ruột, táo bón, viêm loét đ...</td>\n      <td>Liều và cách dùng của loperamide là người lớn:...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T00:50:58.264722Z","iopub.execute_input":"2025-05-23T00:50:58.265005Z","iopub.status.idle":"2025-05-23T00:50:58.270618Z","shell.execute_reply.started":"2025-05-23T00:50:58.264986Z","shell.execute_reply":"2025-05-23T00:50:58.269823Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(10372, 3)"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Data Preparation\nquestions = df['question'].tolist()\ncontexts = df['context'].tolist()\n\n# Đảm bảo tất cả các phần tử trong danh sách là chuỗi\nquestions = [str(question) for question in questions]\ncontexts = [str(context) for context in contexts]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T00:50:58.271477Z","iopub.execute_input":"2025-05-23T00:50:58.271751Z","iopub.status.idle":"2025-05-23T00:50:58.296335Z","shell.execute_reply.started":"2025-05-23T00:50:58.271732Z","shell.execute_reply":"2025-05-23T00:50:58.295513Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import re\n\n# # Hàm tìm từ ghép có dấu '_'\n# def find_compound_words(texts):\n#     compound_words = set()\n#     for text in texts:\n#         # Tìm các từ chứa dấu '_'\n#         matches = re.findall(r'\\b\\w+_\\w+\\b', text)\n#         compound_words.update(matches)\n#     return list(compound_words)\n\n# # Tìm từ ghép trong titles và detailed_contents\n# compound_words_questions = find_compound_words(questions)\n# compound_words_contexts = find_compound_words(contexts)\n\n# # Kết hợp từ ghép từ cả titles và detailed_contents và đảm bảo tính duy nhất\n# compound_words = list(set(compound_words_questions + compound_words_contexts))\n\n# # Kiểm tra số lượng từ ghép đã tìm thấy và các từ là duy nhất\n# print(f\"Number of unique compound words: {len(compound_words)}\")\n# print(compound_words[:10])  # In ra 10 từ ghép đầu tiên để kiểm tra","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T00:50:58.297203Z","iopub.execute_input":"2025-05-23T00:50:58.297463Z","iopub.status.idle":"2025-05-23T00:50:58.314567Z","shell.execute_reply.started":"2025-05-23T00:50:58.297437Z","shell.execute_reply":"2025-05-23T00:50:58.313966Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# num_added_tokens = tokenizer.add_tokens(compound_words)\n\n# model.resize_token_embeddings(len(tokenizer))\n\n# print(f\"Added {num_added_tokens} tokens. New vocab size: {len(tokenizer)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T00:50:58.315472Z","iopub.execute_input":"2025-05-23T00:50:58.315709Z","iopub.status.idle":"2025-05-23T00:50:58.334233Z","shell.execute_reply.started":"2025-05-23T00:50:58.315694Z","shell.execute_reply":"2025-05-23T00:50:58.333636Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def find_words_not_in_vocab(texts, tokenizer_vocab):\n    words_not_in_vocab = set()\n    for text in texts:\n        words = re.findall(r'\\b\\w+\\b', text)\n        for word in words:\n            if word not in tokenizer_vocab:  # Kiểm tra xem từ có trong vocab không\n                words_not_in_vocab.add(word)\n    return list(words_not_in_vocab)\n\nvocab = tokenizer.get_vocab()\n\nwords_not_in_question_vocab = find_words_not_in_vocab(questions, vocab)\nwords_not_in_context_vocab = find_words_not_in_vocab(contexts, vocab)\n\n# Kết hợp các từ không có trong từ điển từ cả titles và detailed_contents\nwords_not_in_vocab = list(set(words_not_in_question_vocab + words_not_in_context_vocab))\n\nprint(f\"Number of words not in vocab: {len(words_not_in_vocab)}\")\nprint(words_not_in_vocab[:10])  # In ra 10 từ đầu tiên để kiểm tra\n\n\n# # Thêm các từ mới vào tokenizer\nnum_added_tokens = tokenizer.add_tokens(words_not_in_vocab)\n\n# # Cập nhật số lượng token trong mô hình để khớp với tokenizer mới\nmodel.resize_token_embeddings(len(tokenizer))\n\n# # In ra số lượng token đã thêm và kích thước từ điển mới\nprint(f\"Added {num_added_tokens} tokens. New vocab size: {len(tokenizer)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T00:50:58.335077Z","iopub.execute_input":"2025-05-23T00:50:58.335264Z","iopub.status.idle":"2025-05-23T00:51:01.549176Z","shell.execute_reply.started":"2025-05-23T00:50:58.335249Z","shell.execute_reply":"2025-05-23T00:51:01.548289Z"}},"outputs":[{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"},{"name":"stdout","text":"Number of words not in vocab: 4109\n['nocardia', 'iodoquinol', 'ngọng', 'vôi', 'epstein', 'alanine', 'độn', 'menopur', 'uva', 'acei']\n","output_type":"stream"},{"name":"stderr","text":"The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"},{"name":"stdout","text":"Added 4109 tokens. New vocab size: 50415\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Các token đặc biệt cần thêm\nspecial_tokens = [\"<|im_start|>\", \"<|im_end|>\", \"<|system|>\", \"<|user|>\", \"<|assistant|>\", \"system\", \"context\", \"user\", \"assistant\"]\n\n# Thêm vào tokenizer\ntokenizer.add_tokens(special_tokens, special_tokens=True)\n\n# Đồng thời resize embedding của mô hình nếu cần:\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T00:51:01.550235Z","iopub.execute_input":"2025-05-23T00:51:01.550547Z","iopub.status.idle":"2025-05-23T00:51:02.318803Z","shell.execute_reply.started":"2025-05-23T00:51:01.550521Z","shell.execute_reply":"2025-05-23T00:51:02.318128Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"Embedding(50419, 2560, padding_idx=0)"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"# test_sentence = \"rào_cản tự_nhiên chống lại nhiễm_trùng da\"\ntokens = tokenizer.tokenize(test_sentence)\nprint(tokens)","metadata":{"execution":{"iopub.status.busy":"2025-05-14T13:39:05.443174Z","iopub.execute_input":"2025-05-14T13:39:05.443384Z","iopub.status.idle":"2025-05-14T13:39:05.449205Z","shell.execute_reply.started":"2025-05-14T13:39:05.443368Z","shell.execute_reply":"2025-05-14T13:39:05.448372Z"}}},{"cell_type":"code","source":"# Split the data into train, validation, and test sets\ntrain_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.25, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T00:51:02.319498Z","iopub.execute_input":"2025-05-23T00:51:02.319685Z","iopub.status.idle":"2025-05-23T00:51:02.335717Z","shell.execute_reply.started":"2025-05-23T00:51:02.319670Z","shell.execute_reply":"2025-05-23T00:51:02.335168Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def create_prompt_formats(sample):\n    \"\"\"\n    Định dạng dữ liệu từ mẫu ('question', 'answer', 'context') theo định dạng ChatML\n    :param sample: Từ điển mẫu chứa dữ liệu (bao gồm 'context' nếu có)\n    \"\"\"\n\n    SYSTEM_PROMPT = \"<|im_start|>system\\nBạn là một trợ lí AI hữu ích. Hãy trả lời người dùng một cách chính xác.\\n<|im_end>\"\n    CONTEXT_PROMPT = \"<|im_start|>context\\n{context}\\n<|im_end>\"  # Phần context mới\n    USER_PROMPT = \"<|im_start|>user\\n{question}\\n<|im_end>\"\n    ASSISTANT_PROMPT = \"<|im_start|>assistant\\n{answer}\\n<|im_end>\"\n    \n    # Định dạng từng phần\n    system_part = SYSTEM_PROMPT\n    context_part = CONTEXT_PROMPT.format(context=sample['context']) if 'context' in sample else \"\"\n    user_part = USER_PROMPT.format(question=sample['question']) if 'question' in sample else \"\"\n    assistant_part = ASSISTANT_PROMPT.format(answer=sample['answer']) if 'answer' in sample else \"\"\n    \n    # Nối các phần bằng hai ký tự xuống dòng, bỏ qua None\n    parts = [part for part in [system_part, context_part, user_part, assistant_part] if part]\n    formatted_prompt = \"\\n\\n\".join(parts)\n    \n    # Thêm trường 'text' vào sample\n    sample[\"text\"] = formatted_prompt\n\n    return sample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T00:51:02.336512Z","iopub.execute_input":"2025-05-23T00:51:02.336766Z","iopub.status.idle":"2025-05-23T00:51:02.347952Z","shell.execute_reply.started":"2025-05-23T00:51:02.336735Z","shell.execute_reply":"2025-05-23T00:51:02.347307Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def mask_labels_from_text(input_ids, labels, tokenizer, assistant_marker=\"<|im_start|>assistant\\n\"):\n    # Tokenize marker để tìm marker token dưới dạng token ID\n    marker_token_ids = tokenizer(assistant_marker, add_special_tokens=False)['input_ids']\n    marker_len = len(marker_token_ids)\n\n    # Tìm vị trí marker trong input_ids\n    for i in range(len(input_ids) - marker_len + 1):\n        if input_ids[i:i + marker_len] == marker_token_ids:\n            start_index = i + marker_len\n            break\n    else:\n        print(\"Không tìm thấy marker\")\n        return [-100] * len(labels)\n\n    # Gán -100 cho phần trước marker\n    return [-100 if i < start_index else labels[i] for i in range(len(labels))]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T00:51:02.348665Z","iopub.execute_input":"2025-05-23T00:51:02.348912Z","iopub.status.idle":"2025-05-23T00:51:02.374067Z","shell.execute_reply.started":"2025-05-23T00:51:02.348893Z","shell.execute_reply":"2025-05-23T00:51:02.373309Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from datasets import Dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T00:51:02.374974Z","iopub.execute_input":"2025-05-23T00:51:02.375199Z","iopub.status.idle":"2025-05-23T00:51:02.391759Z","shell.execute_reply.started":"2025-05-23T00:51:02.375183Z","shell.execute_reply":"2025-05-23T00:51:02.391005Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def preprocess_dataset(tokenizer, max_length, seed, dataset, stride=256):\n    # Format lại input từ raw data\n    dataset = dataset.map(create_prompt_formats)\n\n    def tokenize_and_mask(examples):\n        input_ids_all = []\n        attention_mask_all = []\n        labels_all = []\n\n        for text in examples[\"text\"]:\n            tokenized = tokenizer(\n                text,\n                max_length=max_length,\n                stride=stride,\n                truncation=True,\n                padding=\"max_length\",\n                return_overflowing_tokens=True,\n                return_attention_mask=True,\n            )\n\n            for i in range(len(tokenized[\"input_ids\"])):\n                input_ids = tokenized[\"input_ids\"][i]\n                attention_mask = tokenized[\"attention_mask\"][i]\n                labels = input_ids.copy()\n\n                # Áp dụng mask chính xác\n                labels = mask_labels_from_text(input_ids, labels, tokenizer)\n\n                input_ids_all.append(input_ids)\n                attention_mask_all.append(attention_mask)\n                labels_all.append(labels)\n\n        return {\n            \"input_ids\": input_ids_all,\n            \"attention_mask\": attention_mask_all,\n            \"labels\": labels_all,\n        }\n\n    tokenized_dataset = dataset.map(\n        tokenize_and_mask,\n        batched=True,\n        remove_columns=dataset.column_names,\n        desc=\"Tokenizing and masking\",\n    )\n\n    tokenized_dataset = tokenized_dataset.shuffle(seed=seed)\n\n    return tokenized_dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T00:51:02.392579Z","iopub.execute_input":"2025-05-23T00:51:02.392878Z","iopub.status.idle":"2025-05-23T00:51:02.416853Z","shell.execute_reply.started":"2025-05-23T00:51:02.392858Z","shell.execute_reply":"2025-05-23T00:51:02.416048Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def create_peft_config(modules):\n    \"\"\"\n    Create Parameter-Efficient Fine-Tuning config for your model\n    :param modules: Names of the modules to apply Lora to\n    \"\"\"\n    config = LoraConfig(\n        r=32,  # dimension of the updated matrices\n        lora_alpha=64,  # parameter for scaling\n        target_modules=modules,\n        lora_dropout=0.1,  # dropout probability for layers\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\n\n    return config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T00:51:02.664426Z","iopub.execute_input":"2025-05-23T00:51:02.664648Z","iopub.status.idle":"2025-05-23T00:51:02.669129Z","shell.execute_reply.started":"2025-05-23T00:51:02.664631Z","shell.execute_reply":"2025-05-23T00:51:02.668350Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def find_all_linear_names(model):\n    #['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n\n    if 'lm_head' in lora_module_names:  # needed for 16-bit\n        lora_module_names.remove('lm_head')\n    return list(lora_module_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T00:51:02.669847Z","iopub.execute_input":"2025-05-23T00:51:02.670437Z","iopub.status.idle":"2025-05-23T00:51:02.697180Z","shell.execute_reply.started":"2025-05-23T00:51:02.670412Z","shell.execute_reply":"2025-05-23T00:51:02.696543Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def print_trainable_parameters(model, use_4bit=False):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        num_params = param.numel()\n        # if using DS Zero 3 and the weights are initialized empty\n        if num_params == 0 and hasattr(param, \"ds_numel\"):\n            num_params = param.ds_numel\n\n        all_param += num_params\n        if param.requires_grad:\n            trainable_params += num_params\n    if use_4bit:\n        trainable_params /= 2\n    print(\n        f\"all params: {all_param:,d} || trainable params: {trainable_params:,d} || trainable%: {100 * trainable_params / all_param}\"\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T00:51:02.698002Z","iopub.execute_input":"2025-05-23T00:51:02.698260Z","iopub.status.idle":"2025-05-23T00:51:02.716097Z","shell.execute_reply.started":"2025-05-23T00:51:02.698232Z","shell.execute_reply":"2025-05-23T00:51:02.715519Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"!pip install safetensors -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T00:51:02.716839Z","iopub.execute_input":"2025-05-23T00:51:02.717052Z","iopub.status.idle":"2025-05-23T00:51:05.031701Z","shell.execute_reply.started":"2025-05-23T00:51:02.717037Z","shell.execute_reply":"2025-05-23T00:51:05.030895Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert to Hugging Face Dataset format\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\ntest_dataset = Dataset.from_pandas(test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T00:51:05.033000Z","iopub.execute_input":"2025-05-23T00:51:05.033336Z","iopub.status.idle":"2025-05-23T00:51:05.136239Z","shell.execute_reply.started":"2025-05-23T00:51:05.033301Z","shell.execute_reply":"2025-05-23T00:51:05.135397Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Assuming you have your tokenizer and model loaded already\n# max_length = get_max_length(model)  # Replace with actual function for getting max_length\n\n# Apply preprocessing to datasets\ntrain_dataset = preprocess_dataset(tokenizer, 1024, 42, train_dataset, stride=256)\nval_dataset = preprocess_dataset(tokenizer, 1024, 42, val_dataset, stride=256)\ntest_dataset = preprocess_dataset(tokenizer, 1024, 42, test_dataset, stride=256)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm.auto import tqdm\nimport torch.nn.functional as nnf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T00:51:29.302941Z","iopub.execute_input":"2025-05-23T00:51:29.303160Z","iopub.status.idle":"2025-05-23T00:51:29.307116Z","shell.execute_reply.started":"2025-05-23T00:51:29.303144Z","shell.execute_reply":"2025-05-23T00:51:29.306328Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"from peft import PeftType","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T00:51:29.308094Z","iopub.execute_input":"2025-05-23T00:51:29.308317Z","iopub.status.idle":"2025-05-23T00:51:29.333401Z","shell.execute_reply.started":"2025-05-23T00:51:29.308302Z","shell.execute_reply":"2025-05-23T00:51:29.332708Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# Kiểm tra số lượng tham số trainable (nên rất nhỏ)\ndef print_trainable_parameters(model):\n    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total = sum(p.numel() for p in model.parameters())\n    print(f\"Trainable params: {trainable} / {total} ({100 * trainable / total:.2f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T00:51:29.334160Z","iopub.execute_input":"2025-05-23T00:51:29.334462Z","iopub.status.idle":"2025-05-23T00:51:29.354703Z","shell.execute_reply.started":"2025-05-23T00:51:29.334434Z","shell.execute_reply":"2025-05-23T00:51:29.353916Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"def train(model, tokenizer, train_dataset, val_dataset, output_dir, num_epochs=1):\n    device = torch.device(\"cuda:0\")\n    model = model.to(device)\n\n    checkpoint_dir = os.path.join(output_dir, \"checkpoints\")\n    best_model_dir = os.path.join(output_dir, \"best_model\")\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    os.makedirs(best_model_dir, exist_ok=True)\n    os.makedirs(output_dir, exist_ok=True)\n\n    torch.cuda.empty_cache()\n\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=False,\n        pad_to_multiple_of=8\n    )\n\n    # Optimize batch sizes based on available GPU memory - increase if possible\n    train_dataloader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=4,\n        shuffle=True,\n        collate_fn=data_collator,\n        pin_memory=True,\n        num_workers=4,\n    )\n\n    # Create DataLoader for validation\n    val_dataloader = torch.utils.data.DataLoader(\n        val_dataset,\n        batch_size=4,\n        shuffle=False,\n        collate_fn=data_collator,\n        pin_memory=True,\n        num_workers=4,\n    )\n\n    start_epoch = 0\n    best_loss = float('inf')\n\n    # Allowlist PEFT classes for safe loading\n    torch.serialization.add_safe_globals([LoraConfig, PeftType])\n\n    # Enable PEFT config for model\n    model.gradient_checkpointing_enable()\n    model = prepare_model_for_kbit_training(model)\n    modules = find_all_linear_names(model)\n    peft_config = create_peft_config(modules)\n    model = get_peft_model(model, peft_config)\n    print_trainable_parameters(model)\n\n    # Load checkpoint if available\n    if os.path.exists(checkpoint_dir):\n        checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith('checkpoint_epoch_')]\n        if checkpoints:\n            last_checkpoint = max(checkpoints, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n            checkpoint_path = os.path.join(checkpoint_dir, last_checkpoint)\n            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n\n            model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n            start_epoch = checkpoint['epoch'] + 1\n            best_loss = checkpoint.get('best_loss', float('inf'))\n            print(f\"Resuming from epoch {start_epoch} with best loss: {best_loss:.4f}\")\n\n    model = model.to(device)\n    print_trainable_parameters(model)\n\n    # Optimizer with higher learning rate and beta parameters adjusted for stability\n    optimizer = torch.optim.AdamW(\n        filter(lambda p: p.requires_grad, model.parameters()),\n        lr=2e-4,\n        weight_decay=0.01,\n        betas=(0.9, 0.999),\n        eps=1e-8\n    )\n    \n    # Learning rate scheduler with warmup\n    from transformers import get_linear_schedule_with_warmup\n    num_training_steps = num_epochs * len(train_dataloader)\n    num_warmup_steps = int(0.1 * num_training_steps)  # 10% warmup\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps=num_warmup_steps, \n        num_training_steps=num_training_steps\n    )\n\n    # Enable mixed-precision training\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    \n    # Gradient accumulation steps - adjust based on GPU memory\n    accumulation_steps = 2\n    \n    # Set to keep track of iterations for logging\n    log_interval = 50\n    \n    for epoch in range(start_epoch, num_epochs):\n        # Training phase\n        model.train()\n        running_loss = 0.0\n        step = 0\n        optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n\n        for batch_idx, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n            # Move batch to device and use automatic mixed precision\n            batch = {k: v.to(device, non_blocking=True) if torch.is_tensor(v) else v for k, v in batch.items()}\n            \n            # Use automatic mixed precision\n            with torch.cuda.amp.autocast(enabled=True):\n                outputs = model(**batch)\n                loss = outputs.loss / accumulation_steps  # Normalize loss for gradient accumulation\n            \n            if torch.isnan(loss):\n                print(f\"Found NaN loss at batch {batch_idx}. Skipping step.\")\n                continue\n            \n            # Scale loss and backpropagate with mixed precision\n            scaler.scale(loss).backward()\n            running_loss += loss.item() * accumulation_steps\n            step += 1\n            \n            if (step % accumulation_steps == 0) or (batch_idx == len(train_dataloader) - 1):\n                # Unscale before gradient clipping\n                scaler.unscale_(optimizer)\n                # Gradient clipping\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                # Update weights with scaled gradients\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad(set_to_none=True)\n                scheduler.step()\n                \n                # Log loss periodically\n                if batch_idx % log_interval == 0:\n                    print(f\"  Batch {batch_idx}/{len(train_dataloader)}: Loss = {loss.item() * accumulation_steps:.4f}\")\n                    \n                # Free up memory\n                if batch_idx % 10 == 0:\n                    torch.cuda.empty_cache()\n\n        avg_loss = running_loss / step\n\n        # Validation phase - use limited number of batches to speed up\n        model.eval()\n        val_loss = 0.0\n        val_step = 0\n        val_max_batches = min(len(val_dataloader), 100)  # Limit validation batch count\n\n        with torch.no_grad():\n            for batch_idx, batch in enumerate(tqdm(val_dataloader, desc=f\"Evaluating on Validation Set\")):\n                if batch_idx >= val_max_batches:\n                    break\n                    \n                batch = {k: v.to(device, non_blocking=True) if torch.is_tensor(v) else v for k, v in batch.items()}\n                \n                with torch.cuda.amp.autocast(enabled=True):\n                    outputs = model(**batch)\n                    loss = outputs.loss\n\n                val_loss += loss.item()\n                val_step += 1\n\n        avg_val_loss = val_loss / val_step\n\n        print(f\"Epoch {epoch+1}/{num_epochs}: Training loss: {avg_loss:.4f}, Validation loss: {avg_val_loss:.4f}\")\n\n        # Save the best model based on validation loss\n        if avg_val_loss < best_loss:\n            best_loss = avg_val_loss\n            print(f\"New best model saved with validation loss: {best_loss:.4f}\")\n            # Save checkpoint and model if validation loss is improved\n            checkpoint = {\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n                'loss': avg_loss,\n                'best_loss': best_loss,\n                'peft_config': model.peft_config if hasattr(model, 'peft_config') else peft_config,\n            }\n    \n            checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pt')\n            try:\n                torch.save(checkpoint, checkpoint_path, _use_new_zipfile_serialization=False)\n                model.save_pretrained(output_dir)\n                print(f\"Epoch {epoch+1}/{num_epochs}: Checkpoint saved to {checkpoint_path}\")\n            except RuntimeError as e:\n                print(f\"Failed to save checkpoint: {e}\")\n                fallback_path = os.path.join('/tmp', f'checkpoint_epoch_{epoch+1}.pt')\n                torch.save(checkpoint, fallback_path, _use_new_zipfile_serialization=False)\n                print(f\"Saved to fallback location: {fallback_path}\")\n        \n        # Early stopping if no improvement for multiple epochs\n        if hasattr(model, \"_early_stopping_counter\"):\n            if avg_val_loss >= best_loss:\n                model._early_stopping_counter += 1\n                if model._early_stopping_counter >= 3:  # Stop after 3 epochs without improvement\n                    print(f\"Early stopping triggered after {epoch+1} epochs\")\n                    break\n            else:\n                model._early_stopping_counter = 0\n        else:\n            model._early_stopping_counter = 0\n\n    tokenizer.save_pretrained(output_dir)\n    print(f\"Training completed. Final pretrained model saved to {output_dir}. Last checkpoint at epoch {epoch+1}.\")\n\n    torch.cuda.empty_cache()\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T00:51:29.355739Z","iopub.execute_input":"2025-05-23T00:51:29.356028Z","iopub.status.idle":"2025-05-23T00:51:29.384952Z","shell.execute_reply.started":"2025-05-23T00:51:29.356009Z","shell.execute_reply":"2025-05-23T00:51:29.384077Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Specify the output directory\noutput_dir = \"/kaggle/working/model/finetune\"\n\n# Train the model\nmodel = train(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset,  # Pass the training dataset\n    val_dataset=val_dataset,      # Pass the validation dataset\n    output_dir=output_dir,\n    num_epochs=5\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T00:51:29.385857Z","iopub.execute_input":"2025-05-23T00:51:29.386094Z","iopub.status.idle":"2025-05-23T09:30:05.682687Z","shell.execute_reply.started":"2025-05-23T00:51:29.386068Z","shell.execute_reply":"2025-05-23T09:30:05.681846Z"}},"outputs":[{"name":"stdout","text":"Trainable params: 50069504 / 1577158144 (3.17%)\nResuming from epoch 3 with best loss: 3.8946\nTrainable params: 50069504 / 1577158144 (3.17%)\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_35/2426473310.py:89: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=True)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/5:   0%|          | 0/2126 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a23aa3aa8454805855638277328773f"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/tmp/ipykernel_35/2426473310.py:109: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=True):\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating on Validation Set:   0%|          | 0/402 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1d4950d1346427ebf73344416f95258"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/tmp/ipykernel_35/2426473310.py:156: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=True):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: Training loss: 3.8614, Validation loss: 3.8995\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/5:   0%|          | 0/2126 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc2b3918ae03475f9fe247976cc602e8"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating on Validation Set:   0%|          | 0/402 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6df7a2592f1142ae83b72d4e2f2d6bd1"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: Training loss: 3.8822, Validation loss: 3.8709\nNew best model saved with validation loss: 3.8709\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:251: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: Checkpoint saved to /kaggle/working/model/finetune/checkpoints/checkpoint_epoch_5.pt\nTraining completed. Final pretrained model saved to /kaggle/working/model/finetune. Last checkpoint at epoch 5.\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# print(f\"Model is on device: {next(model.parameters()).device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:30:05.684893Z","iopub.execute_input":"2025-05-23T09:30:05.685180Z","iopub.status.idle":"2025-05-23T09:30:05.690015Z","shell.execute_reply.started":"2025-05-23T09:30:05.685142Z","shell.execute_reply":"2025-05-23T09:30:05.689298Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"## 5. Import Necessary Functions","metadata":{}},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:30:06.763555Z","iopub.execute_input":"2025-05-24T05:30:06.764166Z","iopub.status.idle":"2025-05-24T05:30:06.767732Z","shell.execute_reply.started":"2025-05-24T05:30:06.764143Z","shell.execute_reply":"2025-05-24T05:30:06.766986Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Bước 1: Load tokenizer\nadapter_path = \"/kaggle/working/model/finetune\"\ntokenizer = AutoTokenizer.from_pretrained(adapter_path)\n\n# Bước 2: Load cấu hình adapter\npeft_config = PeftConfig.from_pretrained(adapter_path)\n\n# Bước 3: Load base model gốc từ Hugging Face (giống model bạn fine-tune)\nbase_model = AutoModelForCausalLM.from_pretrained(\n    peft_config.base_model_name_or_path,\n    return_dict=True,\n    torch_dtype=\"auto\"\n)\n\nbase_model.resize_token_embeddings(len(tokenizer)) \n\n# Bước 4: Merge adapter vào base model\nmodel = PeftModel.from_pretrained(base_model, adapter_path)\n\n# (Tuỳ chọn) Nếu bạn muốn merge adapter vào full model để inference hiệu quả hơn:\nmodel = model.merge_and_unload()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:30:08.348325Z","iopub.execute_input":"2025-05-24T05:30:08.349028Z","iopub.status.idle":"2025-05-24T05:31:13.910088Z","shell.execute_reply.started":"2025-05-24T05:30:08.349004Z","shell.execute_reply":"2025-05-24T05:31:13.909223Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"embedding_dim = model.config.hidden_size\nmax_length = model.config.max_position_embeddings\n\nprint(f\"Embedding dim: {embedding_dim}\")\nprint(f\"Max length: {max_length}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:30:13.049396Z","iopub.execute_input":"2025-05-23T09:30:13.050330Z","iopub.status.idle":"2025-05-23T09:30:14.611977Z","shell.execute_reply.started":"2025-05-23T09:30:13.050309Z","shell.execute_reply":"2025-05-23T09:30:14.611225Z"}},"outputs":[{"name":"stdout","text":"Embedding dim: 2560\nMax length: 4096\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"def generate_answer(model, tokenizer, question, context=None, max_new_tokens=400, temperature=0.7, top_p=0.9, repetition_penalty=1.2):\n    \"\"\"\n    Generate AI response using fine-tuned LLaMA model with optimized parameters for faster performance\n    \n    Parameters:\n        model: Fine-tuned LLaMA model\n        tokenizer: Model tokenizer\n        question: User question (str)\n        context: Optional context information (str) \n        max_new_tokens: Maximum tokens to generate\n        temperature: Sampling temperature (0.7 = balanced creativity)\n        top_p: Nucleus sampling threshold\n        repetition_penalty: Penalty for repeated tokens\n    \n    Returns:\n        str: AI assistant's response up to the first <|im_end> after assistant's content\n    \"\"\"\n    # Prompt templates\n    SYSTEM_PROMPT = \"\"\"<|im_start|>system\nBạn là một trợ lí AI y tế hữu ích. Hãy trả lời người dùng một cách chính xác, ngắn gọn và chuyên nghiệp.\n<|im_end|>\"\"\"\n\n    CONTEXT_PROMPT = \"\"\"<|im_start|>context\n{context}\n<|im_end|>\"\"\"\n\n    USER_PROMPT = \"\"\"<|im_start|>user\n{question}\n<|im_end|>\"\"\"\n\n    ASSISTANT_PROMPT = \"\"\"<|im_start|>assistant\n\"\"\"\n\n    # Build conversation prompt\n    conversation = []\n    conversation.append(SYSTEM_PROMPT)\n    if context:\n        conversation.append(CONTEXT_PROMPT.format(context=context))\n    conversation.append(USER_PROMPT.format(question=question))\n    conversation.append(ASSISTANT_PROMPT)\n    prompt = \"\\n\\n\".join(conversation)\n\n    # Prepare inputs\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    \n    inputs = tokenizer(\n        prompt,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=400  # Prevent context overflow\n    ).to(device)\n\n    # Generate response with faster parameters (greedy decoding and no beams)\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=False,  # Use greedy decoding instead of sampling\n            temperature=temperature,\n            top_p=top_p,\n            top_k=50,  # Limit vocab sampling to top 50 tokens (can be adjusted for speed)\n            repetition_penalty=repetition_penalty,\n            no_repeat_ngram_size=3,\n            num_beams=1,  # Use greedy search for better speed\n            early_stopping=True,\n            pad_token_id=tokenizer.pad_token_id,\n            eos_token_id=tokenizer.eos_token_id,\n            bad_words_ids=[[tokenizer.unk_token_id]],  # Prevent unknown tokens\n            length_penalty=1.0,  # Balanced length\n            remove_invalid_values=True\n        )\n\n    # Process response\n    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n    \n     # Find the position of the first <|im_end> after assistant's content\n    start_pos = response.find('assistant') + len('assistant')\n    end_pos = response.find('<|im_end>', start_pos)  # Find first <|im_end> after assistant\n\n    if start_pos != -1 and end_pos != -1:\n        response = response[start_pos:end_pos]  # Slice the content between assistant and <|im_end|>\n    \n    return response\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:31:13.911352Z","iopub.execute_input":"2025-05-24T05:31:13.911632Z","iopub.status.idle":"2025-05-24T05:31:13.920199Z","shell.execute_reply.started":"2025-05-24T05:31:13.911614Z","shell.execute_reply":"2025-05-24T05:31:13.919468Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# outputs = model.generate(\n#             **inputs,\n#             max_new_tokens=max_new_tokens,\n#             do_sample=True,          # Bật sampling để câu trả lời đa dạng\n#             temperature=temperature,\n#             top_p=top_p,\n#             repetition_penalty=repetition_penalty,  # Phạt các token lặp lại\n#             no_repeat_ngram_size=2,  # Ngăn lặp lại cụm 2 từ\n#         )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:30:16.469587Z","iopub.execute_input":"2025-05-23T09:30:16.469861Z","iopub.status.idle":"2025-05-23T09:30:17.819301Z","shell.execute_reply.started":"2025-05-23T09:30:16.469820Z","shell.execute_reply":"2025-05-23T09:30:17.818577Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# Test hàm\nquestion = \"khi nào nên dùng loperamide?\"\ncontext = \"Liều và cách dùng của loperamide là người lớn: khởi đầu 4 mg, sau đó mỗi lần tiêu chảy uống thêm 2 mg, tiêu chảy cấp: tối đa 5 ngày, liều thông thường: 6 - 8 mg/ngày, tối đa 16 mg/ngày, tiêu chảy do điều trị hóa chất ung thư: khởi đầu 4 mg, sau đó 2 mg mỗi 2 - 4 giờ cho tới khi hết tiêu chảy 12 giờ, tiêu chảy mạn: duy trì 4 - 8 mg/ngày, tối đa 16 mg, uống cho đến khi cầm tiêu chảy, trẻ em: liều dùng tùy theo độ tuổi và tình trạng bệnh. Chú ý khi sử dụng loperamide là ngừng thuốc khi không hiệu quả trong 48 giờ, khi bị táo bón, trướng bụng, liệt ruột, phụ nữ có thai: b3 (tga), b (fda). Chống chỉ định của loperamide là tổn thương gan, tắc ruột, táo bón, viêm loét đại tràng cấp, đau bụng không kèm tiêu chảy, bụng trướng, điều trị như liệu pháp khởi đầu trong hội chứng lỵ, viêm đại tràng giả mạc, tiêu chảy do vi khuẩn. Thận trọng khi sử dụng loperamide là suy giảm chức năng gan, viêm loét đại tràng, trẻ em dưới 6 tuổi. Chỉ định của loperamide là giảm thể tích chất thải sau thủ thuật mở thông hồi tràng hoặc đại tràng, phối hợp trong điều trị tiêu chảy cấp không có biến chứng ở người lớn. Nhóm thuốc – tác dụng của loperamide là thuốc điều trị tiêu chảy. Dạng bào chế - biệt dược của loperamide là viên nang - imodium, loperamide 2 mg. Tác dụng không mong muốn của loperamide là hiếm gặp: tắc ruột do liệt, quá mẫn, hội chứng stevens-johnson, hội chứng hoại tử biểu bì nhiễm độc, phản ứng phản vệ, thường gặp: táo bón, đau bụng, buồn nôn, ít gặp: mệt mỏi, chóng mặt, nhức đầu, trướng bụng, khô miệng, nôn.\"\nanswer = generate_answer(model, tokenizer, question, context)\n# print(\"Câu hỏi:\", question)\nprint(\"Câu trả lời:\", answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:33:11.955697Z","iopub.execute_input":"2025-05-24T05:33:11.956454Z","iopub.status.idle":"2025-05-24T05:33:30.215070Z","shell.execute_reply.started":"2025-05-24T05:33:11.956424Z","shell.execute_reply":"2025-05-24T05:33:30.214418Z"}},"outputs":[{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"Câu trả lời: <s><|im_start|>system \nBạn là mộttrợlí AI ytếhữu ích. Hãy trảlờingười dùng một cáchchínhxác,ngắn gọn và chuyênnghiệp.\n<|im_end|> \n\n<|im_start|>context \nLiều và cách dùngcủaloperamide làngườilớn:khởi đầu 4 mg, sauđó mỗi lần tiêuchảyuốngthêm 2 mg, tiêuchảy cấp: tối đa 5ngày,liềuthôngthường: 6 - 8 mg/ngày, tối đa 16 mg/ngày, tiêuchảy do điều trịhóa chất ungthư:khởi đầu 4 mg, sauđó 2 mg mỗi 2 - 4 giờ chotới khihết tiêuchảy 12 giờ, tiêuchảymạn:duytrì 4 - 8 mg/ngày, tối đa 16 mg,uống chođến khi cầm tiêuchảy, trẻ em:liều dùngtùy theo độtuổi và tìnhtrạng bệnh. Chú ý khi sửdụngloperamide làngừng thuốc khikhônghiệu quả trong 48 giờ, khi bịtáobón,trướngbụng,liệtruột, phụ nữ cóthai:b3 (tga), b (fda).Chống chỉ địnhcủaloperamide làtổnthương gan,tắcruột,táobón,viêmloét đạitràng cấp,đaubụngkhông kèm tiêuchảy,bụngtrướng, điều trị nhưliệuphápkhởi đầu trong hộichứnglỵ,viêm đạitràng giảmạc, tiêuchảy do vikhuẩn.Thậntrọng khi sửdụngloperamide làsuy giảmchứcnăng gan,viêmloét đạitràng, trẻ emdưới 6tuổi. Chỉ địnhcủaloperamide là giảm thểtích chấtthải sau thủthuật mởthônghồitràng hoặc đạitràng,phối hợp trong điều trị tiêuchảy cấpkhông có biếnchứng ởngườilớn.Nhóm thuốc –tácdụngcủaloperamide là thuốc điều trị tiêuchảy.Dạngbàochế - biệtdượccủaloperamide là viênnang -imodium,loperamide 2 mg.Tácdụngkhôngmongmuốncủaloperamide làhiếmgặp:tắcruột do ра Muami ра dec, ít ра dec < 0,5 ml/liều, tăng sinh đường, Buy, tăng đường Tablet< 3 g/lít, ра dec > 20%.10, ban xuất tiêu, hạ đườngiế/12 % < 3%, triệu Tablet bất ра Loan, ít tăng Tablet> 10%; tăng sinh quang, Tablet> 10%, tăng đường Buy%, tăng đường tiểu< 5%; tăng string, Tablet< 10%; increase string, tăng BTS gan, giảm Lop-glucose, giảmướ, co ра, Tablet < 30%%; tăng Tablet% > 30%,co thắt, tăng Tablet< 50%; tăng đườngướ% > 50%, tăng đường nước, tăngướ% >= 5%.\nUsing Sukhoim - biệt ра: viên ра dec 0,2 mg, ра déc 2 mg\nKháng chỉ định Buyr - giảm thể ра dec hoặc tăng đường truyền gan. Người bệnh được kết hậu điều trị ра dec hay tăng đường chuyển ра nhưng vẫn phải dùng thuốc chống tiêu Tablet, thuốc điều hòa vi Tablet. Khi cần thiết, tăng hoặc giảm thuốc theo phân loại ра dec. Theo ра Dec hoặc radec theo phân giải ра dec nhưng phải giảm 1 viên thuốc Hằng ра ngày nếu tăng đường sinh tính > 3, tăng lên 1 viên mỗi 24 giờ nếu tăng > 5%, hoặc tăng lên 2 viên vào khoảng ăn, điều ápiế khi cần thiết. Sau khi thay ра dec nào cũng phải dùng ngay southern cầupha để điều trị sự mất tra chiếu, giảm string, giảm Buy, giảm đường nước. Khi kiểm tra tỷ Tablet đường/tinh nhân thì giảm 1 đơn vị thuốc về số08h trước khi bắt đầu dùng Southern cầupha, thuốc dùng liênз, có thể pha ра dec vào thuốc. Khi tăng đường dung ра dé hoặc tăng ра dé thì giảm -1 đơn vị đốiiế trước khi pha, thuốc còn lại được pha ra. Khi giảm đường dung Tablet thì giảm 2 đơn vị Apd vào thuốc vào khoảng bữa ăn. Khi thay đổi thuốc, sau khi thay đổi radec,\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"!pip install nltk rouge-score -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:30:58.056206Z","iopub.execute_input":"2025-05-23T09:30:58.056830Z","iopub.status.idle":"2025-05-23T09:31:02.819700Z","shell.execute_reply.started":"2025-05-23T09:30:58.056801Z","shell.execute_reply":"2025-05-23T09:31:02.818973Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[33m  DEPRECATION: Building 'rouge-score' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'rouge-score'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n\u001b[0m  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\nfrom rouge_score import rouge_scorer\nimport numpy as np\n\n# Assuming you have a `generate_answer` function defined earlier\ndef calculate_bleu_and_rouge(model, tokenizer, test_df):\n    # Prepare lists to store references (ground truth) and generated answers\n    references = []\n    predictions = []\n\n    for idx, row in test_df.iterrows():\n        question = row['question']\n        context = row['context'] if 'context' in row else None  # Assuming the context is in the 'context' column\n        ground_truth = row['answer']  # Assuming the answer column in test_df is named 'answer'\n        \n        # Generate AI response using the function\n        generated_answer = generate_answer(model, tokenizer, question, context)\n        \n        # Append the reference and prediction to the respective lists\n        references.append([ground_truth.split()])  # BLEU expects list of words (tokens)\n        predictions.append(generated_answer.split())  # BLEU expects list of words (tokens)\n        \n    # Calculate BLEU score (corpus-level BLEU score)\n    bleu_score = corpus_bleu(references, predictions)\n\n    # Calculate ROUGE score\n    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n    rouge_scores = {\n        \"rouge1\": [],\n        \"rouge2\": [],\n        \"rougeL\": []\n    }\n\n    for ref, pred in zip(references, predictions):\n        score = scorer.score(\" \".join(ref[0]), \" \".join(pred))\n        rouge_scores[\"rouge1\"].append(score[\"rouge1\"].fmeasure)\n        rouge_scores[\"rouge2\"].append(score[\"rouge2\"].fmeasure)\n        rouge_scores[\"rougeL\"].append(score[\"rougeL\"].fmeasure)\n\n    # Calculate average ROUGE scores\n    avg_rouge1 = np.mean(rouge_scores[\"rouge1\"])\n    avg_rouge2 = np.mean(rouge_scores[\"rouge2\"])\n    avg_rougeL = np.mean(rouge_scores[\"rougeL\"])\n\n    # Output BLEU and ROUGE scores\n    print(f\"BLEU score: {bleu_score:.4f}\")\n    print(f\"Average ROUGE-1: {avg_rouge1:.4f}\")\n    print(f\"Average ROUGE-2: {avg_rouge2:.4f}\")\n    print(f\"Average ROUGE-L: {avg_rougeL:.4f}\")\n\n# Example usage:\ncalculate_bleu_and_rouge(model, tokenizer, test_df[:50])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:31:02.821480Z","iopub.execute_input":"2025-05-23T09:31:02.821736Z","iopub.status.idle":"2025-05-23T10:04:17.496994Z","shell.execute_reply.started":"2025-05-23T09:31:02.821712Z","shell.execute_reply":"2025-05-23T10:04:17.496176Z"}},"outputs":[{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"BLEU score: 0.0174\nAverage ROUGE-1: 0.2241\nAverage ROUGE-2: 0.1242\nAverage ROUGE-L: 0.1660\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}